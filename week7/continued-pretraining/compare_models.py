# -*- coding: utf-8 -*-
"""
Compare baseline â†’ pretrained â†’ finetuned Korean Mistral models (3-way comparison)
Shows progression from original model to final Korean-capable model
"""

import torch
from unsloth import FastLanguageModel
from transformers import TextStreamer

# ANSI color codes for colored output
class Colors:
    HEADER = '\033[95m'
    BLUE = '\033[94m'
    CYAN = '\033[96m'
    GREEN = '\033[92m'
    YELLOW = '\033[93m'
    RED = '\033[91m'
    ENDC = '\033[0m'
    BOLD = '\033[1m'
    UNDERLINE = '\033[4m'

def print_section(title, color=Colors.CYAN):
    """Print a colored section header"""
    print(f"\n{color}{Colors.BOLD}{'='*80}")
    print(f"{title}")
    print(f"{'='*80}{Colors.ENDC}\n")

def load_baseline_model(max_seq_length=2048):
    """Load the original Mistral model (before any training)"""
    model, tokenizer = FastLanguageModel.from_pretrained(
        model_name="unsloth/mistral-7b-v0.3",
        max_seq_length=max_seq_length,
        dtype=None,
        load_in_4bit=True,
    )
    FastLanguageModel.for_inference(model)
    return model, tokenizer

def load_model(model_path, max_seq_length=2048):
    """Load a trained LoRA model"""
    model, tokenizer = FastLanguageModel.from_pretrained(
        model_name=model_path,
        max_seq_length=max_seq_length,
        dtype=None,
        load_in_4bit=True,
    )
    FastLanguageModel.for_inference(model)
    return model, tokenizer

def generate_text(model, tokenizer, prompt, max_new_tokens=150, temperature=0.3):
    """Generate text without streaming"""
    inputs = tokenizer([prompt], return_tensors="pt").to("cuda")
    outputs = model.generate(
        **inputs,
        max_new_tokens=max_new_tokens,
        use_cache=True,
        do_sample=True,
        temperature=temperature,
        pad_token_id=tokenizer.eos_token_id,
    )
    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
    # Remove the prompt from the output
    response = generated_text[len(prompt):].strip()
    return response

def compare_on_prompt(baseline_model, baseline_tokenizer,
                     pretrained_model, pretrained_tokenizer, 
                     finetuned_model, finetuned_tokenizer,
                     prompt, test_name, prompt_translation=None,
                     max_new_tokens=150, temperature=0.3):
    """Compare three models on the same prompt"""
    
    print(f"\n{Colors.BOLD}{'='*80}")
    print(f"{test_name}")
    print(f"{'='*80}{Colors.ENDC}")
    
    if prompt_translation:
        print(f"{Colors.CYAN}Prompt (Translation): {prompt_translation}{Colors.ENDC}\n")
    
    print(f"{Colors.YELLOW}Generating from BASELINE model (original Mistral)...{Colors.ENDC}")
    baseline_output = generate_text(
        baseline_model, baseline_tokenizer, prompt, 
        max_new_tokens, temperature
    )
    
    print(f"{Colors.YELLOW}Generating from PRETRAINED model (after Korean training)...{Colors.ENDC}")
    pretrained_output = generate_text(
        pretrained_model, pretrained_tokenizer, prompt, 
        max_new_tokens, temperature
    )
    
    print(f"{Colors.YELLOW}Generating from FINETUNED model (after instruction tuning)...{Colors.ENDC}")
    finetuned_output = generate_text(
        finetuned_model, finetuned_tokenizer, prompt,
        max_new_tokens, temperature
    )
    
    # Display all three outputs
    print(f"\n{Colors.RED}â”Œâ”€ BASELINE MODEL (Original Mistral) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”{Colors.ENDC}")
    print(f"{Colors.RED}â”‚{Colors.ENDC}")
    for line in baseline_output.split('\n'):
        print(f"{Colors.RED}â”‚{Colors.ENDC} {line}")
    print(f"{Colors.RED}â”‚{Colors.ENDC}")
    print(f"{Colors.RED}â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜{Colors.ENDC}\n")
    
    print(f"{Colors.GREEN}â”Œâ”€ PRETRAINED MODEL (After Korean Wikipedia) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”{Colors.ENDC}")
    print(f"{Colors.GREEN}â”‚{Colors.ENDC}")
    for line in pretrained_output.split('\n'):
        print(f"{Colors.GREEN}â”‚{Colors.ENDC} {line}")
    print(f"{Colors.GREEN}â”‚{Colors.ENDC}")
    print(f"{Colors.GREEN}â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜{Colors.ENDC}\n")
    
    print(f"{Colors.CYAN}â”Œâ”€ FINETUNED MODEL (After Instruction Tuning) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”{Colors.ENDC}")
    print(f"{Colors.CYAN}â”‚{Colors.ENDC}")
    for line in finetuned_output.split('\n'):
        print(f"{Colors.CYAN}â”‚{Colors.ENDC} {line}")
    print(f"{Colors.CYAN}â”‚{Colors.ENDC}")
    print(f"{Colors.CYAN}â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜{Colors.ENDC}\n")

def main():
    print_section("ğŸ”¬ KOREAN MISTRAL 3-WAY MODEL COMPARISON", Colors.HEADER)
    print(f"{Colors.YELLOW}This script compares three model stages:{Colors.ENDC}")
    print(f"  1. {Colors.RED}Baseline{Colors.ENDC} - Original Mistral (no Korean training)")
    print(f"  2. {Colors.GREEN}Pretrained{Colors.ENDC} - After Korean Wikipedia training")
    print(f"  3. {Colors.CYAN}Finetuned{Colors.ENDC} - After instruction tuning")
    print(f"\n{Colors.CYAN}Generation settings: temperature=0.3, do_sample=True (no repetition_penalty){Colors.ENDC}\n")
    
    # Load all three models
    print_section("ğŸ“¥ LOADING MODELS", Colors.BLUE)
    
    print(f"{Colors.YELLOW}Loading baseline model (original Mistral v0.3)...{Colors.ENDC}")
    baseline_model, baseline_tokenizer = load_baseline_model()
    print(f"{Colors.GREEN}âœ“ Baseline model loaded{Colors.ENDC}")
    
    print(f"\n{Colors.YELLOW}Loading pretrained model (after Korean pretraining)...{Colors.ENDC}")
    pretrained_model, pretrained_tokenizer = load_model("lora_model_pretrained")
    print(f"{Colors.GREEN}âœ“ Pretrained model loaded{Colors.ENDC}")
    
    print(f"\n{Colors.YELLOW}Loading finetuned model (after instruction tuning)...{Colors.ENDC}")
    finetuned_model, finetuned_tokenizer = load_model("lora_model")
    print(f"{Colors.GREEN}âœ“ Finetuned model loaded{Colors.ENDC}")
    
    # Define prompts
    wikipedia_prompt_korean = """ìœ„í‚¤í”¼ë””ì•„ ê¸°ì‚¬
### ì œëª©: {}

### ê¸°ì‚¬:
{}"""

    wikipedia_prompt_english = """Wikipedia Article
### Title: {}

### Article:
{}"""

    alpaca_prompt_korean = """ë‹¤ìŒì€ ì‘ì—…ì„ ì„¤ëª…í•˜ëŠ” ëª…ë ¹ì…ë‹ˆë‹¤. ìš”ì²­ì„ ì ì ˆí•˜ê²Œ ì™„ë£Œí•˜ëŠ” ì‘ë‹µì„ ì‘ì„±í•˜ì„¸ìš”.

### ì§€ì¹¨:
{}

### ì‘ë‹µ:
{}"""

    alpaca_prompt_english = """Below is an instruction that describes a task. Write a response that appropriately completes the request.

### Instruction:
{}

### Response:
{}"""
    
    print_section("ğŸ§ª RUNNING 3-WAY COMPARISONS", Colors.CYAN)
    
    # Test 1: Korean Wikipedia
    compare_on_prompt(
        baseline_model, baseline_tokenizer,
        pretrained_model, pretrained_tokenizer,
        finetuned_model, finetuned_tokenizer,
        wikipedia_prompt_korean.format("ì¸ê³µì§€ëŠ¥", ""),
        "Test 1: Korean Wikipedia - Artificial Intelligence (ì¸ê³µì§€ëŠ¥)",
        "Wikipedia Article / Title: Artificial Intelligence / Article:"
    )
    
    # Test 2: English Wikipedia - Preservation Check
    compare_on_prompt(
        baseline_model, baseline_tokenizer,
        pretrained_model, pretrained_tokenizer,
        finetuned_model, finetuned_tokenizer,
        wikipedia_prompt_english.format("Artificial Intelligence", ""),
        "Test 2: English Wikipedia - Artificial Intelligence (Preservation Check)",
        None
    )
    
    # Test 3: Korean Instruction - Kimchi
    compare_on_prompt(
        baseline_model, baseline_tokenizer,
        pretrained_model, pretrained_tokenizer,
        finetuned_model, finetuned_tokenizer,
        alpaca_prompt_korean.format("í•œêµ­ì˜ ì „í†µ ìŒì‹ì¸ ê¹€ì¹˜ì— ëŒ€í•´ ì„¤ëª…í•˜ì„¸ìš”.", ""),
        "Test 3: Korean Instruction - Explain Kimchi",
        "Instruction: Explain about kimchi, a traditional Korean food. / Response:"
    )
    
    # Test 4: Korean Instruction - Seoul
    compare_on_prompt(
        baseline_model, baseline_tokenizer,
        pretrained_model, pretrained_tokenizer,
        finetuned_model, finetuned_tokenizer,
        alpaca_prompt_korean.format("ëŒ€í•œë¯¼êµ­ì˜ ìˆ˜ë„ì¸ ì„œìš¸ì— ëŒ€í•´ ê°„ë‹¨íˆ ì†Œê°œí•´ì£¼ì„¸ìš”.", ""),
        "Test 4: Korean Instruction - Introduce Seoul",
        "Instruction: Briefly introduce Seoul, the capital of South Korea. / Response:"
    )
    
    # Test 5: English Instruction - Preservation Check
    compare_on_prompt(
        baseline_model, baseline_tokenizer,
        pretrained_model, pretrained_tokenizer,
        finetuned_model, finetuned_tokenizer,
        alpaca_prompt_english.format("Explain about Thanksgiving turkey, a traditional American food.", ""),
        "Test 5: English Instruction - Thanksgiving Turkey (Preservation Check)",
        None
    )
    
    print_section("ğŸ“Š COMPARISON COMPLETE", Colors.GREEN)
    
    print(f"{Colors.CYAN}{'='*80}")
    print(f"ğŸ’¡ What to Look For:")
    print(f"{'='*80}{Colors.ENDC}")
    
    print(f"\n{Colors.RED}Baseline Model (Red boxes - Original Mistral):{Colors.ENDC}")
    print(f"  â€¢ Korean: Should be POOR - repetitive, nonsensical")
    print(f"  â€¢ English: Should be GOOD - this is the starting point")
    print(f"  â€¢ Shows what model knows BEFORE any Korean training")
    
    print(f"\n{Colors.GREEN}Pretrained Model (Green boxes - After Korean Wikipedia):{Colors.ENDC}")
    print(f"  â€¢ Korean: Should show IMPROVED fluency and vocabulary")
    print(f"  â€¢ Better Korean sentence structure")
    print(f"  â€¢ Weak instruction-following (only learned language, not how to follow instructions)")
    print(f"  â€¢ English: Should REMAIN strong (no catastrophic forgetting)")
    
    print(f"\n{Colors.CYAN}Finetuned Model (Cyan boxes - After Instruction Tuning):{Colors.ENDC}")
    print(f"  â€¢ Korean: Should be FLUENT with GOOD instruction-following")
    print(f"  â€¢ More structured and complete responses")
    print(f"  â€¢ Directly answers questions")
    print(f"  â€¢ English: Should REMAIN strong")
    
    print(f"\n{Colors.YELLOW}Key Progression to Observe:{Colors.ENDC}")
    print(f"  ğŸ“Š Korean Quality:    {Colors.RED}Poor{Colors.ENDC} â†’ {Colors.GREEN}Better{Colors.ENDC} â†’ {Colors.CYAN}Best{Colors.ENDC}")
    print(f"  ğŸ“Š Instruction:       {Colors.RED}Weak{Colors.ENDC} â†’ {Colors.GREEN}Weak{Colors.ENDC} â†’ {Colors.CYAN}Strong{Colors.ENDC}")
    print(f"  ğŸ“Š English Quality:   {Colors.RED}Good{Colors.ENDC} â†’ {Colors.GREEN}Good{Colors.ENDC} â†’ {Colors.CYAN}Good{Colors.ENDC}")
    print(f"  ğŸ“Š Repetition:        {Colors.RED}High{Colors.ENDC} â†’ {Colors.GREEN}Medium{Colors.ENDC} â†’ {Colors.CYAN}Low{Colors.ENDC}")
    
    print(f"\n{Colors.YELLOW}This demonstrates:{Colors.ENDC}")
    print(f"  âœ“ Continued pretraining successfully teaches new language (Korean)")
    print(f"  âœ“ Instruction tuning teaches how to follow instructions in the new language")
    print(f"  âœ“ English capability is preserved throughout (no catastrophic forgetting)")
    print(f"  âœ“ Both Wikipedia and Instruction tasks show English preservation")
    print(f"  âœ“ Two-stage approach is necessary: language first, then instruction-following")
    print(f"\n{Colors.CYAN}ğŸ’¡ Note: Compare the English tests (Tests 2 & 5) across all three models.")
    print(f"All three should perform similarly well, proving no English degradation.{Colors.ENDC}")
    print()

if __name__ == "__main__":
    main()
